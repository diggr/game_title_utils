{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Corpus from Wikipedia Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create an annotated corpus of video game titles using Wikipedia and DBPedia.\n",
    "\n",
    "For this we need a Wikipedia XML dump (https://en.m.wikipedia.org/wiki/Wikipedia:Database_download).\n",
    "The Wikipedia dump contains 2 files:\n",
    "* enwiki-...-articles-multistream.xml.bz2 (this is the actual content)\n",
    "* enwiki-...-articles-multistream-index.txt.bz2 (a index file containing all page names, an the byte offset of the file-chunk, the page is part of)\n",
    "\n",
    "The index file can be used to extract the correct file-chunk from the bz2 archive (without need to extract the whole archive):\n",
    "\n",
    "Lookup page byte offset -> read file chunk -> extract chunk and prase xml content -> find correct xml entry for page\n",
    "\n",
    "\n",
    "### The general workflow for creating the trainding dataset\n",
    "\n",
    "1. Get a list of all videogame entities (= wikipedia page names) from dbpedia \n",
    "2. Fetch the wikipedia page for each dbpedia entry\n",
    "3. Parse wikipedia page and check each link if it links to a game entry. If so, annotate the link as a game title.\n",
    "4. Build a text corpous with all paragraphs containing one or more game title annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import bz2file\n",
    "import lxml\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, JSONLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBPEDIA SPARQL ENDPOINT\n",
    "SPARQL_ENDPOINT = \"http://dbpedia.org/sparql/\"\n",
    "VIDEO_GAME = \"http://dbpedia.org/ontology/VideoGame\"\n",
    "COMPANY = \"http://dbpedia.org/ontology/Company\"\n",
    "DEVICE = \"http://dbpedia.org/ontology/Device\"\n",
    "GENRE = \"http://dbpedia.org/class/yago/WikicatVideoGameGenres\"\n",
    "REMAKE = \"http://dbpedia.org/class/yago/WikicatVideoGameRemakes\"\n",
    "SOFTWARE = \"http://dbpedia.org/ontology/Software\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia dump files\n",
    "DUMP_PAGES = \"/home/pmuehleder/data/wikipedia/enwiki-20180301-pages-articles-multistream.xml.bz2\"\n",
    "DUMP_INDEX = \"/home/pmuehleder/data/wikipedia/enwiki-20180301-pages-articles-multistream-index.txt.bz2\"\n",
    "\n",
    "OUT_DIR = \"/home/pmuehleder/data/wikipedia/ner_train\"\n",
    "WIKI_DIR = \"/home/pmuehleder/data/wikipedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia dump helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_byte_positions(wiki_idx):\n",
    "    \"\"\"\n",
    "    Returns byte positions, id and title for a specific entry in the wikipedia index dump file\n",
    "    \"\"\"\n",
    "    start_byte = int(wiki_index[wiki_idx].split(\":\")[0])\n",
    "    id_ = int(wiki_index[wiki_idx].split(\":\")[1])\n",
    "    title = wiki_index[wiki_idx].split(\":\")[2]\n",
    "    \n",
    "    #find byte position of next section\n",
    "    offset = 1\n",
    "    while True:\n",
    "        entry = wiki_index[wiki_idx+offset]\n",
    "        end_byte = int(entry.split(\":\")[0])\n",
    "        if end_byte != start_byte:\n",
    "            break\n",
    "        offset += 1\n",
    "    \n",
    "    return start_byte, end_byte, id_, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks():\n",
    "    \"\"\"\n",
    "    Returns byte positions of usable chunks in the wikipedia dump\n",
    "    \"\"\"\n",
    "    current_start_byte = int(wiki_index[0].split(\":\")[0])\n",
    "    for entry in wiki_index:\n",
    "        start_byte = int(entry.split(\":\")[0])\n",
    "        \n",
    "        if start_byte != current_start_byte:\n",
    "            yield (current_start_byte, start_byte)\n",
    "            current_start_byte = start_byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(text):\n",
    "    \"\"\"\n",
    "    Yields text paragraphs of a wiki page.\n",
    "    Removes lists and tables, headlines and paragraphs shorter than 10 tokens.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    for p in paragraphs:\n",
    "        p = p.strip()\n",
    "        if len(p)>0:\n",
    "            if p[0] not in \"#|*=:\":\n",
    "                if len(p.split(\" \")) > 10:\n",
    "                    p = clean(p)\n",
    "                    if \"align=\" not in p and \"[[File:\" not in p:\n",
    "                        yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Removes references (citations, files) and markup text formatting\n",
    "    \"\"\"\n",
    "    text = text.replace(\"''\", \"\").replace(\"{{'}}\", \"'\")\n",
    "    text = re.sub(r'<ref .+?$','', text)\n",
    "    text = re.sub(r'<ref>.+?</ref>', '',text)\n",
    "    text = re.sub(r'<ref name[^<]+?/>', '', text)\n",
    "    text = re.sub(r'<ref name.+?</ref>', '', text)\n",
    "    text = re.sub(r'<.+?>', '', text)\n",
    "    \n",
    "    text = re.sub(r'{{[cC]ite.+?}}', '', text)\n",
    "\n",
    "    text =text.replace(\"{{cite web\", \"\")\n",
    "    #text = re.sub(r'\\[\\[File.+?\\]\\]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Removes wikipedia auto references ( inside '{{ }}' )\n",
    "    \"\"\"\n",
    "    for sub in  re.finditer(r'{{.+?}}', text):\n",
    "        original = sub.group(0)\n",
    "        if len(original.split(\"|\")) > 1:\n",
    "            repl = original.split(\"|\")[1] \n",
    "            if \"{{\" in repl:\n",
    "                repl = \"\"\n",
    "        else:\n",
    "            repl = original.replace(\"{{\",\"\")\n",
    "        repl = repl.replace(\"}}\", \"\")\n",
    "        if \"date=\" in repl:\n",
    "            repl=\"\"\n",
    "        text = text.replace(original, repl)\n",
    "\n",
    "    return text.replace(\"}}\", \"\").replace(\"{{\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunk_for_page(term):\n",
    "    \"\"\"\n",
    "    Looks up wikipedia index for page :term: and returns its byte position in wikipedia dump\n",
    "    \"\"\"\n",
    "    for i, entry in enumerate(wiki_index):\n",
    "        if term in entry:\n",
    "            #print(entry)\n",
    "\n",
    "            positions = get_byte_positions(i)\n",
    "            length = positions[1]-positions[0]\n",
    "            return (positions[0], length, positions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(text):\n",
    "    \"\"\"\n",
    "    Searches all internal wikitext markup links in text and \n",
    "    yields complete link markup string, entity name and link text\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    \n",
    "    for sub in  re.finditer(r'\\[\\[.+?\\]\\]', text):\n",
    "        positions = sub.span()\n",
    "        original = sub.group(0)\n",
    "        \n",
    "        spl = original.split(\"|\")\n",
    "        if len(spl) > 1:\n",
    "            entity = spl[0].replace(\"[[\", \"\")\n",
    "            text = spl[1].replace(\"]]\", \"\")\n",
    "        else:\n",
    "            entity = text = original.replace(\"[[\",\"\").replace(\"]]\",\"\")\n",
    "\n",
    "        entity = entity.replace(\" \",\"_\")\n",
    "        links.append( [original, entity, text, positions] ) \n",
    "    return sorted(links, key=lambda x: x[3][0])\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text):\n",
    "    \"\"\"\n",
    "    checks the links in a \n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for markup, entity, link_text, positions in get_links(text):\n",
    "                    \n",
    "        #text = text.replace(markup, link_text)\n",
    "        start = text.find(markup)\n",
    "        end = start+len(link_text)\n",
    "        \n",
    "        text = text[:start]+link_text+text[start+len(markup):]\n",
    "        if is_video_game(entity):   \n",
    "\n",
    "            annotations.append({\n",
    "                \"type\": \"Game\",\n",
    "                \"name\": link_text,\n",
    "                \"start\": start,\n",
    "                \"end\": end\n",
    "            })\n",
    "\n",
    "            \n",
    "    return text, annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_video_game(entity):\n",
    "    \"\"\"\n",
    "    Queries DBPedia to check if entity is a video game.\n",
    "    A video game is of type \"Videogame\" and \"Software\".\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query.format(entity=entity))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "    except:\n",
    "        print(entity)\n",
    "        return False\n",
    "    types = [ r[\"o\"][\"value\"] for r in results[\"results\"][\"bindings\"] ]\n",
    "\n",
    "    \n",
    "    if VIDEO_GAME in types and SOFTWARE in types:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBPedia SPARQL setup and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(SPARQL_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * \n",
    "WHERE {{\n",
    "  <http://dbpedia.org/resource/{entity}> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o.\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities():\n",
    "    \"\"\"\n",
    "    Fetch all entities from DBPedia of type VideoGame and Software\n",
    "    \"\"\"\n",
    "    \n",
    "    q = \"\"\"\n",
    "    SELECT DISTINCT ?ent\n",
    "    WHERE {{\n",
    "      ?ent ?p ?o.\n",
    "      ?o <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/VideoGame>;\n",
    "         <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://dbpedia.org/ontology/Software> .\n",
    "    }}\n",
    "    OFFSET {offset}\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    sparql.setQuery(q.format(offset=0))\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    entities = [ r[\"ent\"][\"value\"] for r in results[\"results\"][\"bindings\"] ]\n",
    "    \n",
    "    offset = 0\n",
    "    while True:\n",
    "        offset += 10000\n",
    "        print(offset)\n",
    "        ent_count = len(entities)\n",
    "    \n",
    "        sparql.setQuery(q.format(offset=offset))\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        entities += list(set([ r[\"ent\"][\"value\"] for r in results[\"results\"][\"bindings\"] ]))\n",
    "        if ent_count == len(entities):\n",
    "            break\n",
    "    \n",
    "    print(len(set(entities)))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "134598\n"
     ]
    }
   ],
   "source": [
    "vg_entities = get_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_entities[:10]\n",
    "vg_entities_filepath = os.path.join(OUT_DIR, \"vg_entities.json\")\n",
    "with open(vg_entities_filepath, \"w\") as f:\n",
    "    json.dump(vg_entities, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read index file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read index file\n",
    "with bz2.BZ2File(DUMP_INDEX) as f:\n",
    "    wiki_index = f.readlines()\n",
    "\n",
    "wiki_index = [ x.decode().strip() for x in wiki_index ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create annotated NER training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182897"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in get_chunks()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_entities_filepath = os.path.join(WIKI_DIR, \"vg_entities.json\")\n",
    "with open(vg_entities_filepath) as f:\n",
    "    vg_entities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edward_\"Edge\"_Geraldine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:01,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>\n",
      "Tetris & Dr. Mario\n",
      "<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [01:08,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Weird_Al\"_Yankovic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [01:19,  1.93s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "files = os.listdir(OUT_DIR)\n",
    "files = [ int(f.replace(\".json\",\"\")) for f in files ]\n",
    "if len(files):\n",
    "    start_offset = max(files)\n",
    "else:\n",
    "    start_offset = 0\n",
    "\n",
    "#print(start)\n",
    "\n",
    "with open(DUMP_PAGES, \"rb\") as f:\n",
    "    #for byte_start, byte_end in get_chunks():\n",
    "    sents = []\n",
    "    for i, entity in tqdm(enumerate(vg_entities[start_offset:])):\n",
    "        \n",
    "        \n",
    "        #save training data every 100 wiki pages\n",
    "        if (i+1)%100 == 0:\n",
    "            fp = os.path.join(OUT_DIR,\"{}.json\".format(i+1))\n",
    "            with open(fp, \"w\") as of:\n",
    "                json.dump(sents, of, indent=4)\n",
    "            sents = []        \n",
    "        \n",
    "        #check if dbpedia entity is in wikipedia index file and get byte positions of entity\n",
    "        term = entity.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "        try:\n",
    "            byte_start, byte_length, page_title = search_chunk_for_page(term)\n",
    "        except:\n",
    "            print(\">>>\")\n",
    "            print(term)\n",
    "            print(\"<<<\")\n",
    "            continue\n",
    "        \n",
    "        #read and dezip chunk containing wikipedia page \n",
    "        f.seek(byte_start,0)\n",
    "        compressed = f.read(byte_length)\n",
    "        data = bz2.decompress(compressed)\n",
    "        \n",
    "        #iteratate unzipped wiki chunk until page is found\n",
    "        soup = BeautifulSoup(data.decode().strip(), \"lxml\")\n",
    "        for page in soup.find_all(\"page\"):\n",
    "            title = page.find(\"title\").text\n",
    "            if page_title in title:\n",
    "\n",
    "\n",
    "                text = page.find(\"text\").text\n",
    "                text = preprocess(text)\n",
    "\n",
    "                for p in get_paragraphs(text):\n",
    "\n",
    "                    final_text,annotations = annotate(p)\n",
    "                    if len(annotations) > 0:\n",
    "\n",
    "                        sents.append({\n",
    "                            \"text\": final_text,\n",
    "                            \"annotations\": annotations\n",
    "                        })\n",
    "                \n",
    "                break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
